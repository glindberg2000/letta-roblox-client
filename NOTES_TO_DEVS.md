# Notes to Developers - Letta Roblox Integration

## Current Status

We've successfully:
1. Created a working Letta client that supports both free and GPT-4o-mini endpoints
2. Implemented and tested memory management and NPC conversations
3. Set up the server as a systemd service
4. Created comprehensive tests

## Key Findings

### Server Setup
- Pip-installed version (port 8333) works reliably
- Environment variables handle configuration
- Service setup ensures persistence
- Docker version needs different memory structure (currently skipped)

### Client Features
- Default free endpoints work well for testing
- GPT-4o-mini integration works through Letta's API
- Memory management is stable
- Conversation flow is reliable

### Testing
- Basic functionality tests pass
- GPT-4o tests pass
- Memory persistence verified
- Conversation context maintained

## Recommendations

### For Production
1. Use pip-installed Letta server:
   ```bash
   pip install letta
   ```

2. Set up as service with OpenAI key:
   ```bash
   # /etc/systemd/system/letta.service
   Environment=OPENAI_API_KEY=your_key
   ```

3. Use GPT-4o-mini config:
   ```python
   llm_config=LLMConfig(
       model="gpt-4o-mini",
       model_endpoint_type="openai",
       model_endpoint="https://api.openai.com/v1",
       context_window=128000
   )
   ```

### For Development
1. Use free endpoints for testing
2. Run local server on port 8333
3. Use test suite for verification

## Next Steps

### Immediate
1. Set up monitoring
2. Add error handling for network issues
3. Add rate limiting for OpenAI calls

### Future
1. Consider scaling solutions
2. Add metrics collection
3. Implement caching if needed

### Open Questions
1. Do we need Docker support?
2. Should we add more memory types?
3. Do we need conversation history persistence?

## Testing

Run tests:
```bash
# All tests
pytest -sv tests/

# Just GPT-4o tests
pytest -sv tests/test_openai.py
```

## Support

For questions:
1. Check the docs/ directory
2. Run tests with -v flag
3. Check server logs with:
   ```bash
   sudo journalctl -u letta.service -f
   ```

## Notes
- Keep OpenAI key secure
- Monitor usage
- Test memory updates thoroughly
- Consider backup strategies 

## Client Updates

### Key Changes
1. Memory Management
   - Now uses ChatMemory class consistently
   - Memory updates are atomic and validated
   - Memory structure matches Letta's expectations

2. Configuration Flexibility
   ```python
   # Default (free endpoints)
   client = LettaRobloxClient()
   
   # Create agent with GPT-4o-mini
   agent = client.create_agent(
       name="merchant_npc",  # Optional, autogenerated if not provided
       memory=ChatMemory(
           human="Player info here",
           persona="NPC personality here"
       ),
       llm_config=LLMConfig(
           model="gpt-4o-mini",
           model_endpoint_type="openai",
           model_endpoint="https://api.openai.com/v1",
           context_window=128000
       )
   )
   ```

3. Error Handling
   - Better error messages for memory updates
   - Validation of agent responses
   - Automatic cleanup in try/finally blocks

### Breaking Changes
1. Memory Structure
   - Now requires ChatMemory instead of raw dicts
   - Docker version needs different memory format
   - Memory updates use simplified key/value pairs

2. Configuration
   - Removed deprecated config options
   - Server URL must include protocol (http://)
   - Port must be specified in URL

### Usage Tips
1. Memory Management:
   ```python
   # Correct way
   memory = ChatMemory(
       human="Player info here",
       persona="NPC personality here"
   )
   
   # Memory updates
   client.update_memory(agent_id, {
       "human": "New human info",
       "persona": "New persona info"
   })
   ```

2. Best Practices
   - Always use try/finally for cleanup
   - Check memory updates with get_memory()
   - Use default endpoints for testing
   - Keep conversations in context window

3. Common Issues
   - Memory not updating: Check memory structure
   - Agent not responding: Verify message format
   - Server errors: Check environment variables

## Package Structure

1. Dependencies
   ```
   letta-roblox-client
   ‚îú‚îÄ‚îÄ requires letta (core package)
   ‚îÇ   ‚îú‚îÄ‚îÄ ChatMemory
   ‚îÇ   ‚îú‚îÄ‚îÄ LLMConfig
   ‚îÇ   ‚îú‚îÄ‚îÄ EmbeddingConfig
   ‚îÇ   ‚îî‚îÄ‚îÄ create_client
   ‚îÇ
   ‚îî‚îÄ‚îÄ provides
       ‚îú‚îÄ‚îÄ LettaRobloxClient
       ‚îî‚îÄ‚îÄ tools/
   ```

2. Import Structure
   ```python
   # Core classes from letta
   from letta import (
       ChatMemory,     # Memory management
       LLMConfig,      # LLM configuration
       EmbeddingConfig # Embedding configuration
   )
   
   # Our client
   from letta_roblox.client import LettaRobloxClient
   ```

3. Version Requirements
   ```
   letta >= 0.5.5          # For ChatMemory and configs
   requests >= 2.31.0      # For API calls
   python-dotenv >= 1.0.0  # For environment handling
   pytest >= 8.0.0         # For testing
   ```

4. Installation Flow
   ```bash
   # 1. Install core package
   pip install letta>=0.5.5
   
   # 2. Install our client
   pip install letta-roblox-client
   
   # 3. Verify installation
   python -c "from letta import ChatMemory; from letta_roblox.client import LettaRobloxClient"
   ```

5. Development Setup
   ```bash
   # 1. Clone repo
   git clone <repo>
   cd letta-roblox-client
   
   # 2. Create venv
   python -m venv venv
   source venv/bin/activate
   
   # 3. Install dependencies
   pip install letta>=0.5.5
   pip install -e src/
   
   # 4. Run tests
   cd src/
   pytest -sv tests/
   ```

6. Common Issues
   - ModuleNotFoundError for ChatMemory: Install letta package
   - ImportError for LLMConfig: Update letta to >= 0.5.5
   - Missing create_client: Check letta installation
   - Memory structure errors: Use ChatMemory from letta, not dict

### Initialization
The client now supports different initialization modes:

1. API-Only Mode (Default)
   - No database initialization
   - No config loading
   - Minimal side effects
   - Suitable for most Roblox usage

2. Full Mode
   - Initializes database
   - Loads config
   - Creates ~/.letta/
   - Needed for advanced features

Choose based on your needs:
```python
# API calls only
client = LettaRobloxClient()  # skip_init=True by default

# Full features
client = LettaRobloxClient(skip_init=False)
```


üîç **Letta Docker Endpoint Memory Issue**

Hey team! After extensive testing of the Letta client, we've discovered some important differences between the pip-installed server (8333) and Docker endpoint (8283):

### What Works
‚úÖ Pip-installed server (8333):
```python
from letta import ChatMemory
client = LettaRobloxClient("http://localhost:8333")

# This works great!
agent = client.create_agent(
    name="merchant",
    memory=ChatMemory(
        human="Player info",
        persona="NPC personality"
    )
)
```

### What Doesn't Work
‚ùå Docker endpoint (8283):
- Rejects the standard ChatMemory structure
- Returns 422 errors on memory creation
- Different memory validation rules

### Recommendation
For now, we suggest:
1. Use pip-installed server (8333) for development and production
2. Set up as systemd service for stability
3. Skip Docker endpoint until memory structure issues are resolved

### Service Setup
```bash
sudo nano /etc/systemd/system/letta.service

[Unit]
Description=Letta AI Server
After=network.target

[Service]
Type=simple
User=your_user
Environment=OPENAI_API_KEY=your_key
WorkingDirectory=/path/to/letta
ExecStart=/path/to/venv/bin/letta server --port 8333
Restart=always

[Install]
WantedBy=multi-user.target
```

All tests are passing on the pip-installed server, and we've updated the client to default to port 8333. üöÄ

Let me know if you need help migrating from Docker to the pip version!

#letta #development #bugfix

# Server Differences

## Memory Structure
We've discovered important differences between pip-installed and Docker servers:

1. Pip Server (8333)
   ```python
   # Accepts ChatMemory with extra fields
   memory = {
     "memory": {
       "human": {
         "value": "...",
         "limit": 2000,
         "is_template": false,
         "organization_id": null,
         "created_by_id": null,
         "last_updated_by_id": null
       }
     }
   }
   ```

2. Docker Server (8283)
   ```python
   # Requires minimal fields
   memory = {
     "memory": {
       "human": {
         "value": "...",
         "limit": 2000,
         "template": false,
         "user_id": null
       }
     }
   }
   ```

## Other Differences
- Agent Names: Pip uses friendly names, Docker accepts custom names
- Timestamps: Docker uses UTC (Z suffix), Pip uses local time
- Validation: Docker is stricter about extra fields

## Best Practices
1. Use server-appropriate memory format
2. Generate unique names for Docker agents
3. Handle both timestamp formats